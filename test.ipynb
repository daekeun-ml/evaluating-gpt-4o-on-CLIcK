{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "from typing import Tuple\n",
    "\n",
    "import openai\n",
    "from openai import AzureOpenAI, RateLimitError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "from prompts import TYPE_1, TYPE_2, TYPE_3, TYPE_4\n",
    "from logger import logger\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('Korean_LLM_Benchmark_Logger')\n",
    "\n",
    "class CustomStrOutputParser(StrOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        response = text.strip().replace('\"', \"\").replace(\"'\", \"\")\n",
    "        if response.startswith(\"A\"):\n",
    "            pred = \"A\"\n",
    "        elif response.startswith(\"B\"):\n",
    "            pred = \"B\"\n",
    "        elif response.startswith(\"C\"):\n",
    "            pred = \"C\"\n",
    "        elif response.startswith(\"D\"):\n",
    "            pred = \"D\"\n",
    "        elif response.startswith(\"E\"):\n",
    "            pred = \"E\"\n",
    "        else:\n",
    "            pred = \"\"  # Wrong answer\n",
    "\n",
    "        return pred, response\n",
    "    \n",
    "# class CustomOutputParser(BaseOutputParser):\n",
    "#     def parse(self, text: str):\n",
    "#         cleaned_text = text.strip()\n",
    "#         return {\"question\": cleaned_text}\n",
    "    \n",
    "def get_prompt(x) -> str:\n",
    "    num_choices = len(x[\"choices\"])\n",
    "    if num_choices == 4:\n",
    "        if x[\"paragraph\"] != \"\":  # Use Type 1 Prompt\n",
    "            return TYPE_1.format(\n",
    "                CONTEXT=x[\"paragraph\"],\n",
    "                QUESTION=x[\"question\"],\n",
    "                A=x[\"choices\"][0],\n",
    "                B=x[\"choices\"][1],\n",
    "                C=x[\"choices\"][2],\n",
    "                D=x[\"choices\"][3],\n",
    "            )\n",
    "        else:\n",
    "            return TYPE_2.format(\n",
    "                QUESTION=x[\"question\"],\n",
    "                A=x[\"choices\"][0],\n",
    "                B=x[\"choices\"][1],\n",
    "                C=x[\"choices\"][2],\n",
    "                D=x[\"choices\"][3],\n",
    "            )\n",
    "    elif num_choices == 5:\n",
    "        if x[\"paragraph\"] != \"\":\n",
    "            return TYPE_3.format(\n",
    "                CONTEXT=x[\"paragraph\"],\n",
    "                QUESTION=x[\"question\"],\n",
    "                A=x[\"choices\"][0],\n",
    "                B=x[\"choices\"][1],\n",
    "                C=x[\"choices\"][2],\n",
    "                D=x[\"choices\"][3],\n",
    "                E=x[\"choices\"][4],\n",
    "            )\n",
    "        else:\n",
    "            return TYPE_4.format(\n",
    "                QUESTION=x[\"question\"],\n",
    "                A=x[\"choices\"][0],\n",
    "                B=x[\"choices\"][1],\n",
    "                C=x[\"choices\"][2],\n",
    "                D=x[\"choices\"][3],\n",
    "                E=x[\"choices\"][4],\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid number of choices: {num_choices} (ID: {x['id']})\")\n",
    "\n",
    "def get_prompt_template():\n",
    "    system_prompt = \"You are an AI assistant who reads a given question and solves multiple choice questions.\"\n",
    "    system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    human_prompt = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"{question}\"\n",
    "        },\n",
    "    ]\n",
    "    human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            system_message_template,\n",
    "            human_message_template\n",
    "        ]\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def get_answer(x) -> str:\n",
    "    # 왜 이렇게 .strip() 처리를 해주었는지는 README에 issue 파트 참고 부탁드립니다.\n",
    "    answer_idx = [xx.strip() for xx in x[\"choices\"]].index(x[\"answer\"].strip())\n",
    "    if answer_idx == -1:\n",
    "        raise ValueError(f\"Answer not found in choices: {x['answer']} (ID: {x['id']})\")\n",
    "    return chr(0x41 + answer_idx)  # answer_idx = 0 -> answer = \"A\"\n",
    "\n",
    "\n",
    "def get_pred(x) -> Tuple[str, str]:\n",
    "\n",
    "    try:\n",
    "        response = (\n",
    "            client.chat.completions.create(\n",
    "                model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "                messages=[{\"role\": \"user\", \"content\": get_prompt(x)}],\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content.strip()\n",
    "            .replace('\"', \"\")  # Remove double quotes\n",
    "            .replace(\"'\", \"\")  # Remove single quotes\n",
    "        )\n",
    "    except openai.BadRequestError as e:\n",
    "        print(e)\n",
    "        response = \"BAD-REQUEST\"\n",
    "\n",
    "    if response.startswith(\"A\"):\n",
    "        pred = \"A\"\n",
    "    elif response.startswith(\"B\"):\n",
    "        pred = \"B\"\n",
    "    elif response.startswith(\"C\"):\n",
    "        pred = \"C\"\n",
    "    elif response.startswith(\"D\"):\n",
    "        pred = \"D\"\n",
    "    elif response.startswith(\"E\"):\n",
    "        pred = \"E\"\n",
    "    else:\n",
    "        pred = \"\"  # Wrong answer\n",
    "\n",
    "    return pred, response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #MODEL_VERSION = \"gpt-4o-mini\"\n",
    "    MODEL_VERSION = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version    = os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        max_retries    = 3\n",
    "    )\n",
    "\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        temperature=0, \n",
    "        max_tokens=128,\n",
    "        openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_deployment=MODEL_VERSION,                   \n",
    "    )\n",
    "\n",
    "    click_ds = load_dataset(\"EunsuKim/CLIcK\")[\"train\"]\n",
    "\n",
    "    IS_DEBUG = True\n",
    "    if IS_DEBUG:\n",
    "        click_ds = click_ds.select(range(5))\n",
    "\n",
    "    batch_size = 4\n",
    "    MAX_RETRIES = 3\n",
    "    DELAY_INCREMENT = 30\n",
    "\n",
    "    all_questions = [get_prompt(x) for x in tqdm(click_ds)]\n",
    "    all_answers = []\n",
    "    prompt_template = get_prompt_template()\n",
    "    chain = prompt_template | llm | CustomStrOutputParser()\n",
    "\n",
    "    with tqdm(total=len(all_questions), desc=\"Processing Answers\") as pbar:\n",
    "        for i in range(0, len(all_questions), batch_size):\n",
    "            minibatch = all_questions[i:i+batch_size]\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= MAX_RETRIES:\n",
    "                try:\n",
    "                    answers = chain.batch(minibatch, {\"max_concurrency\": batch_size})\n",
    "                    break  # Exit the retry loop once successful\n",
    "                except RateLimitError as rate_limit_error:\n",
    "                    delay = (retries + 1) * DELAY_INCREMENT\n",
    "                    logger.warning(f\"{rate_limit_error}. Retrying in {delay} seconds...\")\n",
    "                    #time.sleep(delay)\n",
    "                    retries += 1\n",
    "\n",
    "                    if retries > MAX_RETRIES:\n",
    "                        logger.error(f\"Max retries reached this batch. Skipping to next batch.\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in process_inputs: {e}\")\n",
    "                    break            \n",
    "            \n",
    "            all_answers.extend(answers)\n",
    "            pbar.set_postfix({\"current_batch\": f\"{i//batch_size + 1}/{(len(all_questions) + (batch_size-1))//batch_size}\"})\n",
    "            pbar.update(len(minibatch))\n",
    "\n",
    "    result = []\n",
    "    for x in tqdm(click_ds):\n",
    "        try:\n",
    "            content = get_prompt(x)\n",
    "            print(content)\n",
    "            answer = get_answer(x)\n",
    "\n",
    "            # for trial in range(3):\n",
    "            #     pred, response = get_pred(x)\n",
    "            #     logger.debug(\n",
    "            #         f\"id: {x['id']} ({trial}), answer: {answer}, pred: {pred}, response: {response}\"\n",
    "            #     )\n",
    "            #     result.append([x[\"id\"], trial, answer, pred, response])\n",
    "        except ValueError as e:\n",
    "            logger.error(e)\n",
    "            continue\n",
    "\n",
    "    # df = pd.DataFrame(result, columns=[\"id\", \"trial\", \"answer\", \"pred\", \"response\"])\n",
    "\n",
    "    # os.makedirs(\"results\", exist_ok=True)\n",
    "    # df.to_csv(f\"results/{MODEL_VERSION}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daekeun/GitHub/evaluating-gpt-4o-on-CLIcK/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 5/5 [00:00<00:00, 2928.98it/s]\n",
      "Processing Answers: 100%|██████████| 5/5 [00:01<00:00,  3.45it/s, current_batch=2/2]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "import openai\n",
    "from openai import AzureOpenAI, RateLimitError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "from prompts import TYPE_1, TYPE_2, TYPE_3, TYPE_4\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "logger = logging.getLogger('Korean_LLM_Benchmark_Logger')\n",
    "\n",
    "class CustomStrOutputParser(StrOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        response = text.strip().replace('\"', \"\").replace(\"'\", \"\")\n",
    "        if response.startswith(\"A\"):\n",
    "            pred = \"A\"\n",
    "        elif response.startswith(\"B\"):\n",
    "            pred = \"B\"\n",
    "        elif response.startswith(\"C\"):\n",
    "            pred = \"C\"\n",
    "        elif response.startswith(\"D\"):\n",
    "            pred = \"D\"\n",
    "        elif response.startswith(\"E\"):\n",
    "            pred = \"E\"\n",
    "        else:\n",
    "            pred = \"\"  # Wrong answer\n",
    "\n",
    "        return pred, response\n",
    "\n",
    "def get_prompt(x) -> str:\n",
    "    num_choices = len(x[\"choices\"])\n",
    "    if num_choices == 4:\n",
    "        if x[\"paragraph\"] != \"\":  # Use Type 1 Prompt\n",
    "            return TYPE_1.format(\n",
    "                CONTEXT=x[\"paragraph\"],\n",
    "                QUESTION=x[\"question\"],\n",
    "                A=x[\"choices\"][0],\n",
    "                B=x[\"choices\"][1],\n",
    "                C=x[\"choices\"][2],\n",
    "                D=x[\"choices\"][3],\n",
    "            )\n",
    "        else:\n",
    "            return TYPE_2.format(\n",
    "                QUESTION=x[\"question\"],\n",
    "                A=x[\"choices\"][0],\n",
    "                B=x[\"choices\"][1],\n",
    "                C=x[\"choices\"][2],\n",
    "                D=x[\"choices\"][3],\n",
    "            )\n",
    "    elif num_choices == 5:\n",
    "        if x[\"paragraph\"] != \"\":\n",
    "            return TYPE_3.format(\n",
    "                CONTEXT=x[\"paragraph\"],\n",
    "                QUESTION=x[\"question\"],\n",
    "                A=x[\"choices\"][0],\n",
    "                B=x[\"choices\"][1],\n",
    "                C=x[\"choices\"][2],\n",
    "                D=x[\"choices\"][3],\n",
    "                E=x[\"choices\"][4],\n",
    "            )\n",
    "        else:\n",
    "            return TYPE_4.format(\n",
    "                QUESTION=x[\"question\"],\n",
    "                A=x[\"choices\"][0],\n",
    "                B=x[\"choices\"][1],\n",
    "                C=x[\"choices\"][2],\n",
    "                D=x[\"choices\"][3],\n",
    "                E=x[\"choices\"][4],\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid number of choices: {num_choices} (ID: {x['id']})\")\n",
    "\n",
    "def get_prompt_template():\n",
    "    system_prompt = \"You are an AI assistant who reads a given question and solves multiple choice questions.\"\n",
    "    system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    human_prompt = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"{question}\"\n",
    "        },\n",
    "    ]\n",
    "    human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            system_message_template,\n",
    "            human_message_template\n",
    "        ]\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def get_answer(x) -> str:\n",
    "    # 왜 이렇게 .strip() 처리를 해주었는지는 README에 issue 파트 참고 부탁드립니다.\n",
    "    answer_idx = [xx.strip() for xx in x[\"choices\"]].index(x[\"answer\"].strip())\n",
    "    if answer_idx == -1:\n",
    "        raise ValueError(f\"Answer not found in choices: {x['answer']} (ID: {x['id']})\")\n",
    "    return chr(0x41 + answer_idx)  # answer_idx = 0 -> answer = \"A\"\n",
    "\n",
    "\n",
    "def benchmark(args):\n",
    "\n",
    "    IS_DEBUG = args.is_debug\n",
    "    batch_size = args.batch_size\n",
    "    MAX_RETRIES = args.max_retries\n",
    "    DELAY_INCREMENT = 30\n",
    "\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        temperature=0, \n",
    "        max_tokens=128,\n",
    "        openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),                   \n",
    "    )\n",
    "\n",
    "    click_ds = load_dataset(\"EunsuKim/CLIcK\")[\"train\"]\n",
    "\n",
    "    if IS_DEBUG:\n",
    "        click_ds = click_ds.select(range(5))\n",
    "\n",
    "    all_batch = [{\"id\": x[\"id\"], \"question\": get_prompt(x), \"answer\": get_answer(x)} for x in tqdm(click_ds)]\n",
    "    responses = []\n",
    "    prompt_template = get_prompt_template()\n",
    "    chain = prompt_template | llm | CustomStrOutputParser()\n",
    "\n",
    "    with tqdm(total=len(all_batch), desc=\"Processing Answers\") as pbar:\n",
    "        for i in range(0, len(all_batch), batch_size):\n",
    "            mini_batch = all_batch[i:i+batch_size]\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= MAX_RETRIES:\n",
    "                try:\n",
    "                    preds = chain.batch(mini_batch, {\"max_concurrency\": batch_size})\n",
    "                    # If no exception, add questions and answers to all_answers\n",
    "                    for qna, pred in zip(mini_batch, preds):\n",
    "                        responses.append({\"id\": qna[\"id\"], \"trial\": 0, \"answer\": qna[\"answer\"], \"pred\": pred[0], \"response\": pred[1]})\n",
    "                    break  # Exit the retry loop once successful\n",
    "                except RateLimitError as rate_limit_error:\n",
    "                    delay = (retries + 1) * DELAY_INCREMENT\n",
    "                    logger.warning(f\"{rate_limit_error}. Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                    retries += 1\n",
    "\n",
    "                    if retries > MAX_RETRIES:\n",
    "                        logger.error(f\"Max retries reached this batch. Skipping to next batch.\")\n",
    "                        break\n",
    "                except openai.BadRequestError as e:\n",
    "                    logger.error(f\"BadRequestError: {e}. Skipping this batch.\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in process_inputs: {e}\")\n",
    "                    break            \n",
    "            \n",
    "            pbar.set_postfix({\"current_batch\": f\"{i//batch_size + 1}/{(len(all_batch) + (batch_size-1))//batch_size}\"})\n",
    "            pbar.update(len(mini_batch))\n",
    "\n",
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Options')\n",
    "\n",
    "    parser.add_argument(\"--is_debug\", type=bool, default=True)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--max_retries\", type=int, default=3)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args)\n",
    "    benchmark(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--is_debug IS_DEBUG]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--max_retries MAX_RETRIES]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/daekeun/Library/Jupyter/runtime/kernel-v2-10936ttYGB3GRGfsN.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daekeun/GitHub/evaluating-gpt-4o-on-CLIcK/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Options')\n",
    "\n",
    "    parser.add_argument(\"--is_debug\", type=bool, default=True)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--max_retries\", type=int, default=3)\n",
    "\n",
    "    args = parser.parse_args()    \n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(responses)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "MODEL_VERSION = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "df.to_csv(f\"results/{MODEL_VERSION}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
